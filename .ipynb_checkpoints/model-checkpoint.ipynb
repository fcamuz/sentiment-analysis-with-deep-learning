{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 2- Modelling\n",
    "\n",
    "This notebook consists the functions and code for modelling.  \n",
    "\n",
    "### CHRISP-DM phases\n",
    "\n",
    "Modelling and Evaluation phases for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 4.Modeling\n",
    "Modeling techniques are selected and applied. \n",
    "\n",
    "#### 5.Evaluation\n",
    "Once one or more models have been built that appear to have high quality based on whichever loss functions have been selected, these need to be tested to ensure they generalize against unseen data and that all key business issues have been sufficiently considered.  The end result is the selection of the champion model(s).\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Modeling With Neural Networks  \n",
    "- 4.Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "np.random.seed(0)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential, Input\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, SpatialDropout1D, Activation, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use (or: number of rows in incoming embedding vector)\n",
    "max_features = 8192\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 128\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smalldata ( ):\n",
    "    # Loading partial train test files , tokenized, sequenzed and padded\n",
    "    pickle_in = open(\"data/vectors_small/X_train2_file.pickle\",\"rb\")\n",
    "    X_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/X_test2_file.pickle\",\"rb\")\n",
    "    X_test2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_train2_file.pickle\",\"rb\")\n",
    "    y_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_test2_file.pickle\",\"rb\")\n",
    "    y_test2 = pickle.load(pickle_in)\n",
    "    \n",
    "    return X_train2, X_test2, y_train2, y_test2\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer=pickle.load(handle)\n",
    "        return tokenizer\n",
    "    \n",
    "def create_model ( hidden_layers, \n",
    "                  loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'],\n",
    "                  embedding_matrix=None,\n",
    "                  max_len=max_len,\n",
    "                  embedding_dims=embedding_dims,\n",
    "                  max_features=max_features,\n",
    "                  glove=False,\n",
    "                 ):\n",
    " \n",
    "    # check if embedding matrix has assigned which means the model uses glove embeddings \n",
    "    if glove==False:\n",
    "        emb_layer=Embedding(input_dim=max_features, input_length=max_len,\n",
    "                        output_dim=embedding_dims)\n",
    "    else:\n",
    "        \n",
    "        emb_layer=Embedding(input_dim =embedding_matrix.shape[0], input_length=max_len,\n",
    "                          output_dim=embedding_matrix.shape[1], \n",
    "                          weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    # instantiate Sequential model\n",
    "    model = Sequential()\n",
    " \n",
    "    # add embedding layer with defined parameters\n",
    "    model.add(emb_layer)\n",
    "   \n",
    "    # add hidden layers available in hidden_layers list\n",
    "    for layer in hidden_layers:\n",
    "        model.add(layer)\n",
    "    \n",
    "    # add pooling layer \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # set the dropout layer to drop out 50% of the nodes\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # add dense layer to produce an output dimension of 50 and using relu activation\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "\n",
    "    # finally add a dense layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    model.summary() \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_model(model, model_name, results, epochs, batch_size=32):\n",
    "    hist = model.fit(X_train2, y_train2, \n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs, \n",
    "                     validation_split=0.1)\n",
    "\n",
    "    test_loss, test_auc = model.evaluate(X_test2, y_test2, batch_size=32)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(model_name + ' Test Loss:    ', test_loss)\n",
    "    print(model_name + ' Test Accuracy:', test_auc)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    #Pass the results as key value pairs to append() function \n",
    "    row=[]\n",
    "    row =[model_name , hist.history['accuracy'][-1],\n",
    "          hist.history['val_accuracy'][-1],test_auc, test_loss] \n",
    "\n",
    "    save_model(model, model_name)\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "def save_model (model, model_name):\n",
    "    path=\"models/\"+model_name + \".h5\"\n",
    "    model.save(path)\n",
    "\n",
    "def save_results (results, row):\n",
    "    #Pass the results as key value pairs to append() function \n",
    "    results = results.append({'model' : row[0] , \n",
    "                        'train_acc' : row[1], \n",
    "                        'val_acc':row[2],\n",
    "                        'test_acc': row[3]\n",
    "                              \n",
    "                                    } , ignore_index=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "### Loading Glove Dictionary\n",
    "def load_glove (path):\n",
    "    # load the glove840B embedding\n",
    "    embeddings_index = dict()\n",
    "    f = open(path)\n",
    "\n",
    "    for line in f:\n",
    "        # Note: use split(' ') instead of split() if you get an error\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # create a weight matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling With Neural Networks                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first try 4 different models to see which one gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe to store the accuarecy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(columns=[\"model\", \"train_acc\",\"val_acc\" ,\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load small dataset to train and test the models\n",
    "X_train2, X_test2, y_train2, y_test2=load_smalldata()\n",
    "#load tokenizer\n",
    "tokenizer=load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters for each model such as hidden layers and glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_params={'hidden_layers':[],\n",
    "            'glove' : False,\n",
    "            'embedding_matrix':None\n",
    "           }\n",
    "\n",
    "\n",
    "cnn_params = {\n",
    "        'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "        'glove' : False,\n",
    "        'embedding_matrix':None}\n",
    "\n",
    "\n",
    "rnn_params ={ \n",
    "     'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "     'glove' : False,\n",
    "     'embedding_matrix':None}\n",
    "\n",
    "#load glove embedding vectors from txt file\n",
    "embedding_matrix=load_glove (\"glove.6B.300d.txt\")\n",
    "\n",
    "cnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "    'glove' : True,\n",
    "    \n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n",
    "\n",
    "rnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "    'glove' : True,\n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for model names and hiden layer\n",
    "names=[\"dnn\",\"cnn\", \"rnn\", \"cnn_glove\", \"rnn_glove\"]\n",
    "params=[dnn_params, cnn_params, rnn_params, \n",
    "        cnn_glove_params, rnn_glove_params]\n",
    "epochs=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For loop for modelling\n",
    "\n",
    "Run a for loop to create the model, compile, fit and save results and the model itsef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set epochs to 3\n",
    "#Run a for loop to create the model, compile, fit and save results \n",
    "#and save the model to models folder\n",
    "for name, param in zip(names, params):\n",
    "    print(\"Model Name :\", name)\n",
    "    print(\"======================\")\n",
    "    model=create_model(hidden_layers=param['hidden_layers'], \n",
    "                   glove=param['glove'],\n",
    "                   embedding_matrix=param['embedding_matrix']\n",
    "                    )\n",
    "    row=run_model(model, name, results, epochs)\n",
    "    results=save_results(results, row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results and picking the model for tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this results CNN is the best performing model so far. From this forward, I will tune only this model and try to impove its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1:  CNN with SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a different optimizer for the same model structure. I will use SGD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.5, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 1 = with sgd optimizer\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None,\n",
    "                      optimizer = sgd)\n",
    "row=run_model(model, \"cnn_glove_sgd\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD optimizer did not perform better than \"adam\". So lets stick with \"adam\" optimizer.  \n",
    "\n",
    "## Iteration 2:  CNN with 2 convolution layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization(),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                        glove =False,\n",
    "                        embedding_matrix=None,\n",
    "                      )\n",
    "epochs=3\n",
    "\n",
    "row=run_model(model, \"cnn_2cnv\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the second convolution layer cnn model did not learn at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 2nd convolution layer did not improved the model. \n",
    "\n",
    "## Iteration 3:  CNN with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None)\n",
    "epochs=8\n",
    "row=run_model(model, \"cnn_glove_8epochs\", results, epochs)\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4:  CNN with different vectorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None)\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/tokens/test_file.pickle\",\"rb\")\n",
    "test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data/tokens/train_file.pickle\",\"rb\")\n",
    "train = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train2=train.sample(n=80000, random_state=1)\n",
    "train2=train2.reset_index()\n",
    "test2=test.sample(n=20000, random_state=1)\n",
    "test2=test2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_split (train, test):\n",
    "    X_train=train.comment\n",
    "    X_test=test.comment\n",
    "    y_train=train.label\n",
    "    y_test=test.label\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split label and comment columns in the data\n",
    "X_train2, X_test2, y_train2, y_test2 = label_split(train2, test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "tfidfvec2 = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "tfidfvec3 = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))\n",
    "countvec = CountVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "countvec2 = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "countvec3 = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"cnn_tfidfvec\",\"cnn_tfidfvec2\", \"cnn_tfidfvec3\", \"cnn_countvec\", \"cnn_countvec2\", \"cnn_countvec3\"]\n",
    "vectorizers=[tfidfvec,tfidfvec2,tfidfvec3,countvec,countvec2,countvec3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def model_performance(vectorizer, train_data, test_data, y_test):\n",
    "accuracy_df = []\n",
    "for name, vectorizer in zip(names, vectorizers):\n",
    "\n",
    "    X_train2 = vectorizer.fit_transform(train2)\n",
    "    X_test2 = vectorizer.transform(test2)\n",
    "\n",
    "    print(\"Model Name :\", name)\n",
    "    print(\"======================\")\n",
    "\n",
    "    row=run_model(model, name, results, epochs)\n",
    "    results=save_results(results, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
