{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 2- Modelling\n",
    "\n",
    "This notebook consists the functions and code for modelling.  \n",
    "\n",
    "### CHRISP-DM phases\n",
    "\n",
    "Modelling and Evaluation phases for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 4.Modeling\n",
    "Modeling techniques are selected and applied. \n",
    "\n",
    "#### 5.Evaluation\n",
    "Once one or more models have been built that appear to have high quality based on whichever loss functions have been selected, these need to be tested to ensure they generalize against unseen data and that all key business issues have been sufficiently considered.  The end result is the selection of the champion model(s).\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1. Import Libraries\n",
    "- 2. Define Functions \n",
    "- 3. Modeling With Neural Networks  \n",
    "- 4. Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "np.random.seed(0)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential, Input\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, SpatialDropout1D, Activation, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use (or: number of rows in incoming embedding vector)\n",
    "max_features = 8192\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 128\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smalldata ( ):\n",
    "    # Loading partial train test files , tokenized, sequenzed and padded\n",
    "    pickle_in = open(\"data/vectors_small/X_train2_file.pickle\",\"rb\")\n",
    "    X_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/X_test2_file.pickle\",\"rb\")\n",
    "    X_test2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_train2_file.pickle\",\"rb\")\n",
    "    y_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_test2_file.pickle\",\"rb\")\n",
    "    y_test2 = pickle.load(pickle_in)\n",
    "    \n",
    "    return X_train2, X_test2, y_train2, y_test2\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer=pickle.load(handle)\n",
    "        return tokenizer\n",
    "    \n",
    "def create_model ( hidden_layers, \n",
    "                  loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'],\n",
    "                  embedding_matrix=None,\n",
    "                  max_len=max_len,\n",
    "                  embedding_dims=embedding_dims,\n",
    "                  max_features=max_features,\n",
    "                  glove=False,\n",
    "                 ):\n",
    " \n",
    "    # check if embedding matrix has assigned which means the model uses glove embeddings \n",
    "    if glove==False:\n",
    "        emb_layer=Embedding(input_dim=max_features, input_length=max_len,\n",
    "                        output_dim=embedding_dims)\n",
    "    else:\n",
    "        \n",
    "        emb_layer=Embedding(input_dim =embedding_matrix.shape[0], input_length=max_len,\n",
    "                          output_dim=embedding_matrix.shape[1], \n",
    "                          weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    # instantiate Sequential model\n",
    "    model = Sequential()\n",
    " \n",
    "    # add embedding layer with defined parameters\n",
    "    model.add(emb_layer)\n",
    "   \n",
    "    # add hidden layers available in hidden_layers list\n",
    "    for layer in hidden_layers:\n",
    "        model.add(layer)\n",
    "    \n",
    "    # add pooling layer \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # set the dropout layer to drop out 50% of the nodes\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # add dense layer to produce an output dimension of 50 and using relu activation\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "\n",
    "    # finally add a dense layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    model.summary() \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_model(model, model_name, results, epochs, batch_size=32):\n",
    "    print(epochs)\n",
    "    hist = model.fit(X_train2, y_train2, \n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs, \n",
    "                     validation_split=0.1)\n",
    "\n",
    "    test_loss, test_auc = model.evaluate(X_test2, y_test2, batch_size=32)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(model_name + ' Test Loss:    ', test_loss)\n",
    "    print(model_name + ' Test Accuracy:', test_auc)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    #Pass the results as key value pairs to append() function \n",
    "    row=[]\n",
    "    row =[model_name , hist.history['accuracy'][-1],\n",
    "          hist.history['val_accuracy'][-1],test_auc, test_loss] \n",
    "\n",
    "    save_model(model, model_name)\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "def save_model (model, model_name):\n",
    "    path=\"models/\"+model_name + \".h5\"\n",
    "    model.save(path)\n",
    "\n",
    "def save_results (results, row):\n",
    "    #Pass the results as key value pairs to append() function \n",
    "    results = results.append({'model' : row[0] , \n",
    "                        'train_acc' : row[1], \n",
    "                        'val_acc':row[2],\n",
    "                        'test_acc': row[3]\n",
    "                              \n",
    "                                    } , ignore_index=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "### Loading Glove Dictionary\n",
    "def load_glove (path):\n",
    "    # load the glove840B embedding\n",
    "    embeddings_index = dict()\n",
    "    f = open(path)\n",
    "\n",
    "    for line in f:\n",
    "        # Note: use split(' ') instead of split() if you get an error\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # create a weight matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling With Neural Networks                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first try 4 different models to see which one gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe to store the accuarecy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(columns=[\"model\", \"train_acc\",\"val_acc\" ,\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load small dataset to train and test the models\n",
    "X_train2, X_test2, y_train2, y_test2=load_smalldata()\n",
    "#load tokenizer\n",
    "tokenizer=load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters for each model such as hidden layers and glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dnn_params={'hidden_layers':[],\n",
    "            'glove' : False,\n",
    "            'embedding_matrix':None\n",
    "           }\n",
    "\n",
    "\n",
    "cnn_params = {\n",
    "        'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "        'glove' : False,\n",
    "        'embedding_matrix':None}\n",
    "\n",
    "\n",
    "rnn_params ={ \n",
    "     'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "     'glove' : False,\n",
    "     'embedding_matrix':None}\n",
    "\n",
    "#load glove embedding vectors from txt file\n",
    "embedding_matrix=load_glove (\"glove.6B.300d.txt\")\n",
    "\n",
    "cnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "    'glove' : True,\n",
    "    \n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n",
    "\n",
    "rnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "    'glove' : True,\n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for model names and hiden layer\n",
    "names=[\"dnn\",\"cnn\", \"rnn\", \"cnn_glove\", \"rnn_glove\"]\n",
    "params=[dnn_params, cnn_params, rnn_params, \n",
    "        cnn_glove_params, rnn_glove_params]\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For loop for modelling\n",
    "\n",
    "Run a for loop to create the model, compile, fit and save results and the model itsef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set epochs to 3\n",
    "#Run a for loop to create the model, compile, fit and save results \n",
    "#and save the model to models folder\n",
    "for name, param in zip(names, params):\n",
    "    model=create_model(hidden_layers=param['hidden_layers'], \n",
    "                   glove=param['glove'],\n",
    "                   embedding_matrix=param['embedding_matrix']\n",
    "                    )\n",
    "    row=run_model(model, name, results, epochs)\n",
    "    results=save_results(results, row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results and picking the model for tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dnn</td>\n",
       "      <td>0.869548</td>\n",
       "      <td>0.876733</td>\n",
       "      <td>0.875133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cnn</td>\n",
       "      <td>0.499652</td>\n",
       "      <td>0.500067</td>\n",
       "      <td>0.503233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>rnn</td>\n",
       "      <td>0.859430</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>0.879867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>cnn_glove</td>\n",
       "      <td>0.859741</td>\n",
       "      <td>0.884067</td>\n",
       "      <td>0.889767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>rnn_glove</td>\n",
       "      <td>0.823593</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.865367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  train_acc   val_acc  test_acc\n",
       "0        dnn   0.869548  0.876733  0.875133\n",
       "1        cnn   0.499652  0.500067  0.503233\n",
       "2        rnn   0.859430  0.882800  0.879867\n",
       "3  cnn_glove   0.859741  0.884067  0.889767\n",
       "4  rnn_glove   0.823593  0.861000  0.865367"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this results CNN with Glove Embedding is the best performing model so far. From this forward, I will tune only this model and try to impove its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1:  CNN with SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a different optimizer for the same model structure. I will use SGD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.5, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 128, 300)          264087600 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_15 (Spatia (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 128, 100)          120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_22 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 264,213,201\n",
      "Trainable params: 125,401\n",
      "Non-trainable params: 264,087,800\n",
      "_________________________________________________________________\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/3\n",
      "135000/135000 [==============================] - 352s 3ms/step - loss: 0.5606 - accuracy: 0.6943 - val_loss: 0.3936 - val_accuracy: 0.8351\n",
      "Epoch 2/3\n",
      "135000/135000 [==============================] - 327s 2ms/step - loss: 0.4389 - accuracy: 0.8011 - val_loss: 0.3822 - val_accuracy: 0.8483\n",
      "Epoch 3/3\n",
      "135000/135000 [==============================] - 335s 2ms/step - loss: 0.4149 - accuracy: 0.8147 - val_loss: 0.3558 - val_accuracy: 0.8517\n",
      "30000/30000 [==============================] - 29s 982us/step\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "cnn_glove_sgd Test Loss:     0.35046087396144865\n",
      "cnn_glove_sgd Test Accuracy: 0.8560000061988831\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iteration 1 = with sgd optimizer\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                   \n",
    "                      glove = True,\n",
    "                      embedding_matrix=embedding_matrix,\n",
    "                      optimizer = sgd)\n",
    "row=run_model(model, \"cnn_glove_sgd\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD optimizer seems slightly better performing than \"adam\". So lets use SGD.  \n",
    "\n",
    "## Iteration 2:  CNN with 2 convolution layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 128, 300)          264087600 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_16 (Spatia (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 128, 100)          120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 128, 100)          40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_23 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 264,253,701\n",
      "Trainable params: 165,701\n",
      "Non-trainable params: 264,088,000\n",
      "_________________________________________________________________\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/3\n",
      "135000/135000 [==============================] - 590s 4ms/step - loss: 0.4616 - accuracy: 0.7734 - val_loss: 0.3252 - val_accuracy: 0.8574\n",
      "Epoch 2/3\n",
      "135000/135000 [==============================] - 514s 4ms/step - loss: 0.3683 - accuracy: 0.8404 - val_loss: 0.3076 - val_accuracy: 0.8675\n",
      "Epoch 3/3\n",
      "135000/135000 [==============================] - 502s 4ms/step - loss: 0.3400 - accuracy: 0.8543 - val_loss: 0.2851 - val_accuracy: 0.8791\n",
      "30000/30000 [==============================] - 42s 1ms/step\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "cnn_glove_2cnv Test Loss:     0.28176174082756045\n",
      "cnn_glove_2cnv Test Accuracy: 0.8811666369438171\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iteration 1 = with sgd optimizer\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization(),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                        glove = True,\n",
    "                        embedding_matrix=embedding_matrix,\n",
    "                      )\n",
    "row=run_model(model, \"cnn_glove_2cnv\", results)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 2nd convolution layer did not improved the model. \n",
    "\n",
    "### Iteration 3:  CNN_Glove  with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 128, 300)          17691000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_15 (Spatia (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 128, 100)          120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 17,816,601\n",
      "Trainable params: 125,401\n",
      "Non-trainable params: 17,691,200\n",
      "_________________________________________________________________\n",
      "10\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 323s 2ms/step - loss: 0.6999 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5001\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 323s 2ms/step - loss: 0.6934 - accuracy: 0.5001 - val_loss: 0.6936 - val_accuracy: 0.5001\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 323s 2ms/step - loss: 0.6934 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.5001\n",
      "Epoch 4/10\n",
      "127456/135000 [===========================>..] - ETA: 17s - loss: 0.6934 - accuracy: 0.5003"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d5c0a4e59935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       embedding_matrix=embedding_matrix)\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn_glove_8epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4d48a8ca78b0>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, model_name, results, epochs, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                      validation_split=0.1)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iteration 1 = with sgd optimizer\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = True,\n",
    "                      embedding_matrix=embedding_matrix)\n",
    "epochs=8\n",
    "row=run_model(model, \"cnn_glove_8epochs\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
