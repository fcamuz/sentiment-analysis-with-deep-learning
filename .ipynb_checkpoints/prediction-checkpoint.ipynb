{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 3- Prediction\n",
    "\n",
    "This notebook consists the functions and code for scraping comments from Amazon website, pre-prosessing them and prediction with the model.  \n",
    "\n",
    "### CHRISP-DM phase\n",
    "\n",
    "Deployment phase for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 6.Deployment\n",
    "\n",
    "Generally this will mean deploying a code representation of the model into an operating system to score or categorize new unseen data as it arises and to create a mechanism for the use of that new information in the solution of the original business problem. Importantly, the code representation must also include all the data prep steps leading up to modeling so that the model will treat new raw data in the same manner as during model development.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Scraping Comments\n",
    "- 4.Pre-processing the New Data\n",
    "- 5.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model, Sequential, Input\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('cnn_2cnv.h5')\n",
    "\n",
    "model = joblib.load(\"models/joblib_RL_Model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 100)          25700     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 555,489\n",
      "Trainable params: 555,289\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n",
    "\n",
    "Open the saved tokenizer with pickle. This tokenizer was trained in the pre-prosessing notebook and saved with pickle. We need this for creating the pipe line for the new data we will scrape from Amazon website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews ():\n",
    "    \"\"\"This function scrapes customer reviews from Amazon web \n",
    "    site at a given product page URL. Uses input method to receive\n",
    "    the URL from the user\"\"\"\n",
    "    # For ignoring SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    url=input(\"Enter Amazon Product Url- \")\n",
    "    html_page = urllib.request.urlopen(url) #Make a get request to retrieve the page\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    reviews=[]\n",
    "    ratings=[]\n",
    "\n",
    "    review_row=soup.findAll('div', attrs={'data-hook': 'review'})\n",
    "    for row in review_row:\n",
    "        ratings.append(row.find('span',  attrs={'class':'a-icon-alt'}).text.strip()[0])\n",
    "        reviews.append(row.find('div', attrs={'data-hook': 'review-collapsed'}).text.strip())\n",
    "    \n",
    "    print('There are {} reviews in for this product on this page'.format(len(reviews)) )\n",
    "    #print(reviews)\n",
    "    \n",
    "    return reviews, ratings\n",
    "\n",
    "def punctuationRemover(p):\n",
    "    '''\n",
    "    Input: Takes a string. You may have to use str() to force it. \n",
    "    Removes all punctuation by checking every single character.\n",
    "    Output: Returns a string.\n",
    "    '''\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890''' \n",
    "    no_punctuations = ''\n",
    "\n",
    "    for words in p: # You may not have to loop this high\n",
    "        for char in words:\n",
    "            if char in punctuations:\n",
    "                no_punctuations = no_punctuations + ' '\n",
    "            if char not in punctuations:\n",
    "                no_punctuations = no_punctuations + char    \n",
    "    return(no_punctuations)\n",
    "\n",
    "\n",
    "def removeStopWords(str):\n",
    "    \"\"\"it takes a string and removes the stopwords from it. \n",
    "    Stopwords are available in the \"stop\" list\"\"\"\n",
    "    #select english stopwords\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    #add custom words\n",
    "    stop.update(('arnt','this','when','cant','these'))\n",
    "    #remove stop words\n",
    "    new_str = ' '.join([word.lower() for word in str if word.lower() not in stop]) \n",
    "    return new_str\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Comments \n",
    "\n",
    "In this part we will scrape the comments from the website for a product. We will also scrape the rating to evaluate the model's prediction fro unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Amazon Product Url- https://www.amazon.com/Amplified-Digital-Antenna-Skywire-Antennas/dp/B07DK1M5JF/ref=lp_3230976011_1_2_sspa?s=tv&ie=UTF8&qid=1582172255&sr=1-2-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExUTFQT0xCUVdHOERXJmVuY3J5cHRlZElkPUEwMjY5OTU4OE5BTFdMQVoyNUEmZW5jcnlwdGVkQWRJZD1BMDk4NzQ2MzNPRFQ5SUJDNVVINEYmd2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3NlJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==\n",
      "There are 6 reviews in for this product on this page\n"
     ]
    }
   ],
   "source": [
    "review_list, ratings = scrape_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like that this antenna is light weight.--- It has a long chord that will reach windows where you can get the most of your viewing experiences...--- The picture quality is nice and works great with HD broadcasting.--- It has a large receptor plane which is nice BC it is attractive and never an eye sore!--- If you spend the time to understand the technology- you will grow to love this NIFTY antenna...--- The antenna works well with my LG smart TV, the TV has CH+ so the Antenna is more than able to comply with all demands.We love the football games on our 43\" TV- the Picture quality- is outstanding and the refresh rate is never a problem with this antenna...The antenna works well with my LG smart TV, the TV has CH+ so the Antenna is more than able to comply with all demands.I also like that the seller provides excellent services after the sell.', 'Great amplified had digital antennae. It helps us get the local stations and some others in clearly. It‚Äôs easy to set up. We are pleased with this purchase.', 'ü§ìWe love how easy to install this antenna is!We were able to hide it very nicely behind our real size \"Van Gogh\" üòåWe were surprised how even with the antenna working so well even hidden behind the painting , we were afraid of having an ugly black square in the middle of a white wall! ü§™We got many channels without the cable billüòâ , it is great with the sports channels and news ones! the image was really nice and sharp all the time!ü§©üëçüèªTip: to know which channels you can get go to \"antenna web dot org\" üòéüíÉüï∫Great, great antenna! the price was more than excellent! Cheers!', 'First of all let me tell you I am mostly a streamer and much prefer the choices and variety of the likes of Netflix, Hulu and the like. I have had been a subscriber of Cable TV services in the past but with the ever increasing pricing In relation to the amount of garbage that‚Äôs on regular TV today I made the decision to cut the cord once and for all.What came out of my cord cutting endeavor not only saves me money for stuff I never watch but allows me to invest more of my free time being able to not only watch my local channels for FREE but also gives me the freedom to binge on what I actually like on streaming services without paying hundreds of dollars to make the cable company rich.If you are looking for a good antenna, this one is a good choice. If you want to cut the cord as well just remember that having a TV antenna depends on a factor of things such as distance from the transmitter of the station down to where you live and where you have the antenna placed. Results may vary but mine are very satisfactory.I myself get 33 channels all with good or very good reception. I have my antenna placed behind a picture and it works pretty well. I am not sure about range in comparison to the description but it works well for me. See the video for more information of what to expect but keep in mind that results may vary where you live or have it placed.This antenna DOES seem to work better that others I have tried and it stays out of the way where nobody can see it. Only thing I don‚Äôt like about it and would like to see changed in a new version would be to have a white antenna and white wire going down my wall instead of a black wire. It‚Äôs not a complete deal breaker and can probably be covered up though.', 'We have a tv we move around for different needs and wanted to be able to use it in the garage to watch sports while still doing projects - this simple and easy to use antenna met our needs perfectly.  I thought I may need my husband to help set it up but it was pretty simple just plug it in, run through the menu options and in under a minute I was able to watch a number of channel easily.', 'Easy to install.  Able to pick up a couple more stations than the one had.  Also gets my local station good.  Picture quality good.  Beats paying high cable fees.'] ['5', '5', '5', '5', '5', '5']\n"
     ]
    }
   ],
   "source": [
    "print(review_list, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-process the New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopwords and punctuation from the comment by using the pre-processing functions. Also split in to words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_punc=[punctuationRemover(p) for p in review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_stop=[removeStopWords(p) for p in review_list_no_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=review_list_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vektor=tokenizer.texts_to_sequences(test)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 675, 1100, 1718,  466,  845,  512,  535,  675,  466, 4186,  535,\n",
       "        1285,  535,  890,  890,  803,  890, 1718,  466,  675,  466, 1129,\n",
       "         675,  466, 1718,  675,  466,  890,  466, 1100,  466,  675,  845,\n",
       "         675,  435, 1285,  466,  816, 1718, 1718,  890,  466,  890,  890,\n",
       "        1718,  466,  890,  466,  890,  890,  816,  675,  988,  816,  466,\n",
       "        1285, 1285,  816, 1718, 1285,  803,  675, 1100, 1718,  466, 1100,\n",
       "        1718,  512, 1718, 1718,  466,  890,  466,  890,  890,  675,  466,\n",
       "        1718,  890,  435, 1285,  466,  512,  845, 1285,  816, 1718, 1285,\n",
       "        1285,  466,  890, 1285, 1285,  988,  466, 1718, 1718,  466,  466,\n",
       "        1285, 1285,  466,  675,  845,  675, 1100,  466,  466,  531,  512,\n",
       "         466, 1285, 1285,  466,  890,  466,  675, 1100,  512,  466, 1129,\n",
       "         466,  675, 1718,  466,  466, 1285, 1285],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  803,  675,  466,  845, 1285, 1129,  466, 1718,  803,\n",
       "        1285,  890,  466,  890,  890,  466, 1718,  466, 1285,  845,  535,\n",
       "         803,  466, 1718,  466, 1285,  512, 1285,  890,  890,  466, 1718,\n",
       "         466,  675,  890,  512, 1285,  466,  675, 1285,  466,  466,  535,\n",
       "         845,  816,  466,  675,  466,  845, 1285,  466,  466,  816, 1718,\n",
       "        1718,  845,  535,  675,  512, 1718,  466],\n",
       "       [1285,  816,  466,  803,  890,  512, 1718,  890,  890,  466, 1285,\n",
       "         816, 1718,  535, 1718,  466,  512,  435, 1285,  466,  435, 1285,\n",
       "        1285,  803,  675,  466,  816, 1718, 1718,  466,  845,  675,  512,\n",
       "        1718,  890,  890,  466, 1285,  890,  890,  466,  816,  890,  466,\n",
       "        1718,  466,  803,  466,  816,  675,  466, 1285, 1285,  890,  512,\n",
       "         466,  890, 1718,  675,  845, 1285, 1285, 1718,  466,  466,  845,\n",
       "         988,  890,  816,  816, 1718,  512, 1718,  512, 1718,  890,  890,\n",
       "         466, 1285,  535,  512,  890,  803,  466,  803,  890,  466,  890,\n",
       "         890,  816,  466,  435,  675,  803,  803,  675,  466,  803,  675,\n",
       "         466,  890,  466,  890,  890, 1718,  466,  845,  675,  512,  466,\n",
       "         816,  675,  466, 1718,  890,  466,  531,  512,  466, 1285, 1285,\n",
       "         466,  890,  512, 1718,  466,  466,  675],\n",
       "       [ 466,  675,  466,  890,  535, 1129, 1718,  466,  816,  816, 1718,\n",
       "         466,  675,  466,  890,  435,  512,  890,  466,  466,  890, 1285,\n",
       "        1718,  890,  803,  890, 1285,  988,  466,  435,  535,  890,  816,\n",
       "         535, 1285, 1285,  988,  466,  466,  466,  512, 1718,  890,  803,\n",
       "         466,  890,  890,  466,  816, 1100,  466,  675,  890,  816,  535,\n",
       "        1285,  435,  466, 1718, 1100,  466,  816, 1718,  466,  890,  466,\n",
       "         890,  890,  890,  816, 1718,  466,  816,  675,  466,  803,  890,\n",
       "         803,  816,  890,  816, 1285, 1285,  890,  466, 1129,  435, 1285,\n",
       "         512,  988,  816,  675,  466,  890,  512,  845, 1285,  466,  466,\n",
       "         466, 1285,  435,  675,  466,  988,  466,  675,  890,  512,  890,\n",
       "         845,  675,  435,  435, 1285,  435,  466,  512, 1100,  466,  675,\n",
       "         466,  535,  845, 1718,  535,  803, 1718],\n",
       "       [1718,  466,  803,  675,  803,  466,  816,  512, 1718,  845,  675,\n",
       "         816, 1718, 1285,  466, 1285, 1285,  890,  803,  845,  675, 1419,\n",
       "         466,  512, 1718,  845, 1285,  466,  890,  466,  535,  466,  890,\n",
       "         466,  890,  890,  466,  535,  675,  890,  466,  466,  845,  466,\n",
       "         675, 1129,  466,  512, 1285, 1718,  535,  803, 1718,  890,  466,\n",
       "         466, 1718,  535,  435,  890, 1718,  466, 1285,  845,  466,  535,\n",
       "         845,  435,  535,  816,  845,  675,  466,  845, 1285,  466, 1419,\n",
       "         535,  845, 1285,  535,  803,  890,  675,  535,  890, 1718,  675,\n",
       "         535,  803, 1718, 1718,  466,  466,  890,  535,  845,  890,  890,\n",
       "         890,  535,  890,  466,  675,  890,  535,  466,  816,  435, 1285,\n",
       "         466,  816,  512, 1718,  890,  535,  435,  466,  675, 1129,  512,\n",
       "        1718,  890,  890,  466, 1285,  466, 1285],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,  466,  890, 1285, 1285,  435, 1285,  466,  845,\n",
       "         512,  988,  535,  845,  512,  535,  845, 1285,  466,  675,  466,\n",
       "         890, 1718,  890, 1718,  466,  890,  466, 1718, 1285,  803,  466,\n",
       "        1285,  512, 1285,  890,  803,  845,  512,  535,  675,  466, 4186,\n",
       "         535, 1285,  803,  435,  466,  845,  890,  803, 1718,  803, 1718,\n",
       "         512,  435, 1285,  466, 1129,  466,  466]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict([test_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([test_vector]))\n",
    "print([round(prediction[0][0]) for prediction[0][0] in prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and predicting multiple customer reviews from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_comments ():\n",
    "    review_list, ratings = scrape_reviews() \n",
    "    review_list_no_punc=[punctuationRemover(p) for p in review_list]\n",
    "    review_list_no_stop=[removeStopWords(p) for p in review_list_no_punc]\n",
    "    test=review_list_no_stop\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    prediction=model.predict([test_vector])\n",
    "    \n",
    "    neg_com=[]\n",
    "    pos_com=[]\n",
    "    label=[int(round(prediction[0][0])) for prediction[0][0] in prediction]\n",
    "    labels=[]\n",
    "    for i in label:\n",
    "        if i==1:\n",
    "            labels.append('P')\n",
    "        else: \n",
    "            labels.append('N')\n",
    "        \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='N':\n",
    "            neg_com.append(review_list[j])\n",
    "            \n",
    "            \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='P':\n",
    "            pos_com.append(review_list[j])\n",
    "   \n",
    "    print (\"\")\n",
    "    print (\"Predictions\")\n",
    "    print(labels)\n",
    "    print (\"Actual Rates\")\n",
    "    print(ratings)\n",
    "    print (\"\")\n",
    "    \n",
    "    print(\"List of negative comments\")\n",
    "    print(\"============================\")\n",
    "    for i in neg_com:\n",
    "        print (i)\n",
    "        print (\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Amazon Product Url- https://www.amazon.com/Amplified-Digital-Antenna-Skywire-Antennas/dp/B07DK1M5JF/ref=lp_3230976011_1_2_sspa?s=tv&ie=UTF8&qid=1582172255&sr=1-2-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExUTFQT0xCUVdHOERXJmVuY3J5cHRlZElkPUEwMjY5OTU4OE5BTFdMQVoyNUEmZW5jcnlwdGVkQWRJZD1BMDk4NzQ2MzNPRFQ5SUJDNVVINEYmd2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3NlJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==\n",
      "There are 6 reviews in for this product on this page\n",
      "\n",
      "Predictions\n",
      "['P', 'P', 'P', 'P', 'P', 'P']\n",
      "Actual Rates\n",
      "['5', '5', '5', '5', '5', '5']\n",
      "\n",
      "List of negative comments\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "web_comments()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a function that scrapes comments from a product page on Amazon website.\n",
    "\n",
    "You can easily use this model for classification for any amazon product. \n",
    "All you need is;\n",
    "\n",
    "- Run the web-comments  function\n",
    "- It will ask you to input the amazon product page URL\n",
    "- Just copy and paste the url and press ENTER\n",
    "\n",
    "The function will give you how many comments are there in total. It also labels each one of them for you as P or N. \n",
    "And filters the Negative one so that you would see why your customer is complaining .\n",
    "\n",
    "To get valuable insight to revise your customer service you would read and  analyze  only 3 comments instead of 20.\n",
    "\n",
    "I provides huge time and money saving in the long run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "***Work in progress***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the tone of a single comment such as email or text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_comment ():\n",
    "    comment=input(\"Enter comment here- \")\n",
    "    \n",
    "    comment_nopunc=[punctuationRemover(comment)]\n",
    "    comment_no_stop= \"\".join([word.lower() for word in comment_nopunc if  word.lower() not in (stop)])\n",
    "    print(comment_no_stop)\n",
    "\n",
    "    test=comment_no_stop\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    print(test_vektor)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    print(test_vector)\n",
    "    prediction=model.predict([test_vector])\n",
    "\n",
    "    print (int(round(prediction[0][0])))\n",
    "\n",
    "single_comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment=input(\"Enter comment here- \")\n",
    "\n",
    "comment=[\"I love this product!\"]\n",
    "    \n",
    "comment_nopunc=[punctuationRemover(comment)]\n",
    "comment_no_stop= \"\".join([word.lower() for word in comment_nopunc if  word.lower() not in (stop)])\n",
    "print(comment_no_stop)\n",
    "\n",
    "test=comment_no_stop\n",
    "test_vektor=tokenizer.texts_to_sequences (test)                             \n",
    "print(test_vektor)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "prediction=model.predict([test_vector])\n",
    "\n",
    "print (int(round(prediction[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Web App for the model \n",
    "\n",
    "***Work in progress***\n",
    "\n",
    "For customer use I would  Create a web app out of this prediction model.\n",
    "Web scraping can be adapted to any product website other than Amazon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the prediction to Anvil \n",
    "I would like to run this prediction as a web app. I used one interface to get input and give the results to the user. It is called Anvil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"O5X2QNJXLPWEQ2MQIAJTAQHP-AYZTXNLHKK3ZZOB2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def say_hello(name):\n",
    "  print(\"Hello from the uplink, %s!\" % name)\n",
    "\n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "@anvil.server.collable\n",
    "def sentiment(file):\n",
    "    with anvil.media.Tempfile(file) as filename:\n",
    "        text = url_box(filename)\n",
    "        \n",
    "        \n",
    "    test_vektor=tokenizer.texts_to_sequences(text)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    \n",
    "    score=model.predict(text_vector)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ipython nbconvert --to FORMAT notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem in the connection. It kept throwing errors on the Anvil platform. So moved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin app with SPYRE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyre import server\n",
    "\n",
    "app=server.App()\n",
    "\n",
    "app.Launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spyre\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement spyre (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for spyre\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a version missmatch here I guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLFlow for machine learnig life cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to redo this project in this a platform to see the difference and also for easy deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
