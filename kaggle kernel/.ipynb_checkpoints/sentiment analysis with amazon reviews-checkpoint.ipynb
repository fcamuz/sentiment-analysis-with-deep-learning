{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/X_test_file.pickle\n",
      "/kaggle/input/y_train_file.pickle\n",
      "/kaggle/input/y_test_file.pickle\n",
      "/kaggle/input/X_train_file.pickle\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test_file.pickle', 'y_train_file.pickle', 'y_test_file.pickle', 'X_train_file.pickle']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_pickle('../input/y_train_file.pickle')\n",
    "X_train=pd.read_pickle('../input/X_train_file.pickle')\n",
    "y_test=pd.read_pickle('../input/y_test_file.pickle')\n",
    "X_test=pd.read_pickle('../input/X_test_file.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600000,)\n",
      "(3600000, 128)\n"
     ]
    }
   ],
   "source": [
    "print (y_train.shape)\n",
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential, Input\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, SpatialDropout1D, Activation, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use\n",
    "max_features = 8192\n",
    "\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 128\n",
    "\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate CNN model\n",
    "cnn_2cnv = Sequential()\n",
    "\n",
    "# add embedding layer \n",
    "cnn_2cnv.add(Embedding(input_dim=max_features, input_length=max_len,\n",
    "                        output_dim=embedding_dims))\n",
    " \n",
    "# set the dropout layer to drop out 50% of the nodes\n",
    "cnn_2cnv.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# add convolutional layer that has ...\n",
    "# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\n",
    "cnn_2cnv.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n",
    "\n",
    "# add normalization layer\n",
    "cnn_2cnv.add(BatchNormalization())\n",
    "\n",
    "# add convolutional layer that has ...\n",
    "# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\n",
    "cnn_2cnv.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n",
    "\n",
    "# add normalization layer\n",
    "cnn_2cnv.add(BatchNormalization())\n",
    "\n",
    "# add pooling layer \n",
    "cnn_2cnv.add(GlobalMaxPool1D())\n",
    "\n",
    "# set the dropout layer to drop out 50% of the nodes\n",
    "cnn_2cnv.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer to produce an output dimension of 50 and using relu activation\n",
    "cnn_2cnv.add(Dense(50, activation='relu'))\n",
    "# finally add a dense layer\n",
    "cnn_2cnv.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 100)          25700     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 100)          40100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 595,989\n",
      "Trainable params: 595,589\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_2cnv.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_2cnv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240000 samples, validate on 360000 samples\n",
      "Epoch 1/8\n",
      "3240000/3240000 [==============================] - 3219s 994us/step - loss: 0.2760 - accuracy: 0.8817 - val_loss: 0.2206 - val_accuracy: 0.9118\n",
      "Epoch 2/8\n",
      "3240000/3240000 [==============================] - 3235s 998us/step - loss: 0.2331 - accuracy: 0.9061 - val_loss: 0.2106 - val_accuracy: 0.9157\n",
      "Epoch 3/8\n",
      "3240000/3240000 [==============================] - 3257s 1ms/step - loss: 0.2251 - accuracy: 0.9099 - val_loss: 0.2124 - val_accuracy: 0.9157\n",
      "Epoch 4/8\n",
      "3240000/3240000 [==============================] - 3184s 983us/step - loss: 0.2199 - accuracy: 0.9121 - val_loss: 0.2029 - val_accuracy: 0.9193\n",
      "Epoch 5/8\n",
      "3240000/3240000 [==============================] - 3294s 1ms/step - loss: 0.2164 - accuracy: 0.9137 - val_loss: 0.2015 - val_accuracy: 0.9201\n",
      "Epoch 6/8\n",
      "3240000/3240000 [==============================] - 3277s 1ms/step - loss: 0.2138 - accuracy: 0.9149 - val_loss: 0.2012 - val_accuracy: 0.9208\n",
      "Epoch 7/8\n",
      "2546176/3240000 [======================>.......] - ETA: 11:16 - loss: 0.2110 - accuracy: 0.9160"
     ]
    }
   ],
   "source": [
    "cnn_2cnv_hist = cnn_2cnv.fit(X_train, y_train, batch_size=512, \n",
    "                         epochs=8, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(cnn_2cnv, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2cnv_loss, cnn_2cnv_auc = cnn_2cnv.evaluate(X_test, y_test, batch_size=32)\n",
    "print('Test Loss:    ', cnn_2cnv_loss)\n",
    "print('Test Accuracy:', cnn_2cnv_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2cnv.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib_file = \"joblib_RL_Model.pkl\"  \n",
    "joblib.dump(cnn_2cnv, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "cnn_model = joblib.load(joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2cnv.evaluate(X_test, y_test, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
