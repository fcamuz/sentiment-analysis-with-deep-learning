{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/X_test_file.pickle\n/kaggle/input/y_train_file.pickle\n/kaggle/input/y_test_file.pickle\n/kaggle/input/X_train_file.pickle\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":2,"outputs":[{"output_type":"stream","text":"['X_test_file.pickle', 'y_train_file.pickle', 'y_test_file.pickle', 'X_train_file.pickle']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=pd.read_pickle('../input/y_train_file.pickle')\nX_train=pd.read_pickle('../input/X_train_file.pickle')\ny_test=pd.read_pickle('../input/y_test_file.pickle')\nX_test=pd.read_pickle('../input/X_test_file.pickle')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (y_train.shape)\nprint (X_train.shape)","execution_count":4,"outputs":[{"output_type":"stream","text":"(3600000,)\n(3600000, 128)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential, Input\nfrom keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, SpatialDropout1D, Activation, LSTM\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text, sequence\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\nfrom keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\nfrom keras.optimizers import Adam\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of unique words we want to use\nmax_features = 8192\n\n# max number of words in a comment to use (or: number of columns in incoming embedding vector)\nmax_len = 128\n\n# dimension of the embedding variable (or: number of rows in output of embedding vector)\nembedding_dims = 64","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate CNN model\ncnn_2cnv = Sequential()\n\n# add embedding layer \ncnn_2cnv.add(Embedding(input_dim=max_features, input_length=max_len,\n                        output_dim=embedding_dims))\n \n# set the dropout layer to drop out 50% of the nodes\ncnn_2cnv.add(SpatialDropout1D(0.5))\n\n# add convolutional layer that has ...\n# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\ncnn_2cnv.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n\n# add normalization layer\ncnn_2cnv.add(BatchNormalization())\n\n# add convolutional layer that has ...\n# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\ncnn_2cnv.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n\n# add normalization layer\ncnn_2cnv.add(BatchNormalization())\n\n# add pooling layer \ncnn_2cnv.add(GlobalMaxPool1D())\n\n# set the dropout layer to drop out 50% of the nodes\ncnn_2cnv.add(Dropout(0.5))\n\n# add dense layer to produce an output dimension of 50 and using relu activation\ncnn_2cnv.add(Dense(50, activation='relu'))\n# finally add a dense layer\ncnn_2cnv.add(Dense(1, activation='sigmoid'))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_2cnv.compile(loss='binary_crossentropy',\n                  optimizer=Adam(0.01),\n                  metrics=['accuracy'])\n\ncnn_2cnv.summary()","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 128, 64)           524288    \n_________________________________________________________________\nspatial_dropout1d_1 (Spatial (None, 128, 64)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 128, 100)          25700     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 128, 100)          400       \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 128, 100)          40100     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 128, 100)          400       \n_________________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 100)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                5050      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 595,989\nTrainable params: 595,589\nNon-trainable params: 400\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_2cnv_hist = cnn_2cnv.fit(X_train, y_train, batch_size=512, \n                         epochs=8, validation_split=0.1)","execution_count":null,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 3240000 samples, validate on 360000 samples\nEpoch 1/8\n3240000/3240000 [==============================] - 3219s 994us/step - loss: 0.2760 - accuracy: 0.8817 - val_loss: 0.2206 - val_accuracy: 0.9118\nEpoch 2/8\n3240000/3240000 [==============================] - 3235s 998us/step - loss: 0.2331 - accuracy: 0.9061 - val_loss: 0.2106 - val_accuracy: 0.9157\nEpoch 3/8\n3240000/3240000 [==============================] - 3257s 1ms/step - loss: 0.2251 - accuracy: 0.9099 - val_loss: 0.2124 - val_accuracy: 0.9157\nEpoch 4/8\n3240000/3240000 [==============================] - 3184s 983us/step - loss: 0.2199 - accuracy: 0.9121 - val_loss: 0.2029 - val_accuracy: 0.9193\nEpoch 5/8\n3240000/3240000 [==============================] - 3294s 1ms/step - loss: 0.2164 - accuracy: 0.9137 - val_loss: 0.2015 - val_accuracy: 0.9201\nEpoch 6/8\n3240000/3240000 [==============================] - 3277s 1ms/step - loss: 0.2138 - accuracy: 0.9149 - val_loss: 0.2012 - val_accuracy: 0.9208\nEpoch 7/8\n2546176/3240000 [======================>.......] - ETA: 11:16 - loss: 0.2110 - accuracy: 0.9160","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'finalized_model.sav'\npickle.dump(cnn_2cnv, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_2cnv_loss, cnn_2cnv_auc = cnn_2cnv.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', cnn_2cnv_loss)\nprint('Test Accuracy:', cnn_2cnv_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_2cnv.save('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib_file = \"joblib_RL_Model.pkl\"  \njoblib.dump(cnn_2cnv, joblib_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\ncnn_model = joblib.load(joblib_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_2cnv.evaluate(X_test, y_test, batch_size=32)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}