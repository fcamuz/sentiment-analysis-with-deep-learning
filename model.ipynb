{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 2- Modelling\n",
    "\n",
    "This notebook consists the functions and code for modelling.  \n",
    "\n",
    "### CHRISP-DM phases\n",
    "\n",
    "Modelling and Evaluation phases for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 4.Modeling\n",
    "Modeling techniques are selected and applied. \n",
    "\n",
    "#### 5.Evaluation\n",
    "Once one or more models have been built that appear to have high quality based on whichever loss functions have been selected, these need to be tested to ensure they generalize against unseen data and that all key business issues have been sufficiently considered.  The end result is the selection of the champion model(s).\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Modeling With Neural Networks  \n",
    "- 4.Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "np.random.seed(0)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential, Input\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM, SpatialDropout1D, Activation, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\n",
    "from keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words we want to use (or: number of rows in incoming embedding vector)\n",
    "max_features = 8192\n",
    "# max number of words in a comment to use (or: number of columns in incoming embedding vector)\n",
    "max_len = 128\n",
    "# dimension of the embedding variable (or: number of rows in output of embedding vector)\n",
    "embedding_dims = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smalldata ( ):\n",
    "    # Loading partial train test files , tokenized, sequenzed and padded\n",
    "    pickle_in = open(\"data/vectors_small/X_train2_file.pickle\",\"rb\")\n",
    "    X_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/X_test2_file.pickle\",\"rb\")\n",
    "    X_test2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_train2_file.pickle\",\"rb\")\n",
    "    y_train2 = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"data/vectors_small/y_test2_file.pickle\",\"rb\")\n",
    "    y_test2 = pickle.load(pickle_in)\n",
    "    \n",
    "    return X_train2, X_test2, y_train2, y_test2\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer=pickle.load(handle)\n",
    "        return tokenizer\n",
    "    \n",
    "def create_model ( hidden_layers, \n",
    "                  loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'],\n",
    "                  embedding_matrix=None,\n",
    "                  max_len=max_len,\n",
    "                  embedding_dims=embedding_dims,\n",
    "                  max_features=max_features,\n",
    "                  glove=False,\n",
    "                 ):\n",
    " \n",
    "    # check if embedding matrix has assigned which means the model uses glove embeddings \n",
    "    if glove==False:\n",
    "        emb_layer=Embedding(input_dim=max_features, input_length=max_len,\n",
    "                        output_dim=embedding_dims)\n",
    "    else:\n",
    "        \n",
    "        emb_layer=Embedding(input_dim =embedding_matrix.shape[0], input_length=max_len,\n",
    "                          output_dim=embedding_matrix.shape[1], \n",
    "                          weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    # instantiate Sequential model\n",
    "    model = Sequential()\n",
    " \n",
    "    # add embedding layer with defined parameters\n",
    "    model.add(emb_layer)\n",
    "   \n",
    "    # add hidden layers available in hidden_layers list\n",
    "    for layer in hidden_layers:\n",
    "        model.add(layer)\n",
    "    \n",
    "    # add pooling layer \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # set the dropout layer to drop out 50% of the nodes\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # add dense layer to produce an output dimension of 50 and using relu activation\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "\n",
    "    # finally add a dense layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    model.summary() \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_model(model, model_name, results, epochs, batch_size=32):\n",
    "    hist = model.fit(X_train2, y_train2, \n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs, \n",
    "                     validation_split=0.1)\n",
    "\n",
    "    test_loss, test_auc = model.evaluate(X_test2, y_test2, batch_size=32)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(model_name + ' Test Loss:    ', test_loss)\n",
    "    print(model_name + ' Test Accuracy:', test_auc)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    #Pass the results as key value pairs to append() function \n",
    "    row=[]\n",
    "    row =[model_name , hist.history['accuracy'][-1],\n",
    "          hist.history['val_accuracy'][-1],test_auc, test_loss] \n",
    "\n",
    "    save_model(model, model_name)\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "def save_model (model, model_name):\n",
    "    path=\"models/\"+model_name + \".h5\"\n",
    "    model.save(path)\n",
    "\n",
    "def save_results (results, row):\n",
    "    #Pass the results as key value pairs to append() function \n",
    "    results = results.append({'model' : row[0] , \n",
    "                        'train_acc' : row[1], \n",
    "                        'val_acc':row[2],\n",
    "                        'test_acc': row[3]\n",
    "                              \n",
    "                                    } , ignore_index=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "### Loading Glove Dictionary\n",
    "def load_glove (path):\n",
    "    # load the glove840B embedding\n",
    "    embeddings_index = dict()\n",
    "    f = open(path)\n",
    "\n",
    "    for line in f:\n",
    "        # Note: use split(' ') instead of split() if you get an error\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # create a weight matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling With Neural Networks                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first try 4 different models to see which one gives the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe to store the accuarecy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(columns=[\"model\", \"train_acc\",\"val_acc\" ,\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load small dataset to train and test the models\n",
    "X_train2, X_test2, y_train2, y_test2=load_smalldata()\n",
    "#load tokenizer\n",
    "tokenizer=load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters for each model such as hidden layers and glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_params={'hidden_layers':[],\n",
    "            'glove' : False,\n",
    "            'embedding_matrix':None\n",
    "           }\n",
    "\n",
    "\n",
    "cnn_params = {\n",
    "        'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "        'glove' : False,\n",
    "        'embedding_matrix':None}\n",
    "\n",
    "\n",
    "rnn_params ={ \n",
    "     'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "     'glove' : False,\n",
    "     'embedding_matrix':None}\n",
    "\n",
    "#load glove embedding vectors from txt file\n",
    "embedding_matrix=load_glove (\"glove.6B.300d.txt\")\n",
    "\n",
    "cnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                   Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                   BatchNormalization()],\n",
    "    'glove' : True,\n",
    "    \n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n",
    "\n",
    "rnn_glove_params ={\n",
    "    'hidden_layers':[SpatialDropout1D(0.5),\n",
    "                  Bidirectional(LSTM(25, \n",
    "                  return_sequences=True))],\n",
    "    'glove' : True,\n",
    "    'embedding_matrix':embedding_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for model names and hiden layer\n",
    "names=[\"dnn\",\"cnn\", \"rnn\", \"cnn_glove\", \"rnn_glove\"]\n",
    "params=[dnn_params, cnn_params, rnn_params, \n",
    "        cnn_glove_params, rnn_glove_params]\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For loop for modelling\n",
    "\n",
    "Run a for loop to create the model, compile, fit and save results and the model itsef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name : dnn\n",
      "======================\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 527,589\n",
      "Trainable params: 527,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "135000/135000 [==============================] - 92s 682us/step - loss: 0.3937 - accuracy: 0.8245 - val_loss: 0.3016 - val_accuracy: 0.8722\n",
      "30000/30000 [==============================] - 2s 63us/step\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "dnn Test Loss:     0.30914589029947914\n",
      "dnn Test Accuracy: 0.8706666827201843\n",
      "\n",
      "\n",
      "\n",
      "Model Name : cnn\n",
      "======================\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 100)          25700     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 555,489\n",
      "Trainable params: 555,289\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "135000/135000 [==============================] - 251s 2ms/step - loss: 0.6968 - accuracy: 0.5010 - val_loss: 0.6935 - val_accuracy: 0.4988\n",
      "30000/30000 [==============================] - 13s 434us/step\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "cnn Test Loss:     0.6933107460975647\n",
      "cnn Test Accuracy: 0.5032333135604858\n",
      "\n",
      "\n",
      "\n",
      "Model Name : rnn\n",
      "======================\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 128, 64)           524288    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128, 50)           18000     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 544,889\n",
      "Trainable params: 544,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "135000/135000 [==============================] - 946s 7ms/step - loss: 0.6271 - accuracy: 0.6366 - val_loss: 0.4808 - val_accuracy: 0.7775\n",
      "30000/30000 [==============================] - 40s 1ms/step\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "rnn Test Loss:     0.48579550501505536\n",
      "rnn Test Accuracy: 0.7744666934013367\n",
      "\n",
      "\n",
      "\n",
      "Model Name : cnn_glove\n",
      "======================\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 128, 300)          263894100 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 128, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 128, 100)          120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 264,019,701\n",
      "Trainable params: 125,401\n",
      "Non-trainable params: 263,894,300\n",
      "_________________________________________________________________\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      " 32768/135000 [======>.......................] - ETA: 5:46 - loss: 0.6083 - accuracy: 0.6867"
     ]
    }
   ],
   "source": [
    "#set epochs to 3\n",
    "#Run a for loop to create the model, compile, fit and save results \n",
    "#and save the model to models folder\n",
    "for name, param in zip(names, params):\n",
    "    print(\"Model Name :\", name)\n",
    "    print(\"======================\")\n",
    "    model=create_model(hidden_layers=param['hidden_layers'], \n",
    "                   glove=param['glove'],\n",
    "                   embedding_matrix=param['embedding_matrix']\n",
    "                    )\n",
    "    row=run_model(model, name, results, epochs)\n",
    "    results=save_results(results, row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results and picking the model for tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this results CNN is the best performing model so far. From this forward, I will tune only this model and try to impove its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1:  CNN with SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a different optimizer for the same model structure. I will use SGD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.5, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 1 = with sgd optimizer\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None,\n",
    "                      optimizer = sgd)\n",
    "row=run_model(model, \"cnn_glove_sgd\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD optimizer did not perform better than \"adam\". So lets stick with \"adam\" optimizer.  \n",
    "\n",
    "## Iteration 2:  CNN with 2 convolution layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization(),\n",
    "                        Conv1D(filters=100,kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                        glove =False,\n",
    "                        embedding_matrix=None,\n",
    "                      )\n",
    "epochs=3\n",
    "\n",
    "row=run_model(model, \"cnn_2cnv\", results, epochs)\n",
    "\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the second convolution layer cnn model performed slighly better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Iteration 3:  CNN with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None)\n",
    "epochs=8\n",
    "row=run_model(model, \"cnn_glove_8epochs\", results, epochs)\n",
    "results=save_results(results, row)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained 5 models as base models and choose the best performing one to tune. Since deep learning models need a lot of computational power and  memory, I have used small sample from the dataset to train these model . Those 5 are :  \n",
    "\n",
    "- Dense Neural Network\n",
    "- Convolutional Neural Network\n",
    "- Recurring Neural Network\n",
    "- CNN with Glove Embedding\n",
    "- RNN with Glove Embedding\n",
    "\n",
    "Glove embedding is a pre-trained dictionary that would help for my model . But in the end CNN model was the best performing one. \n",
    "\n",
    "I have tuned convolution neural network. My laptop’s CPU could not handle the  training with full dataset. So I trained the model on Kaggle. Kaggle is a website that provides a platform for us to deal with massive dataset and heavy computations for machine learning. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "I would compare the different word scoring methods for tuning my model. I used ‘binary’’  but there are others such as “count”, “tfidf”, “freq” Those methods are used to select the most impactful words that would be used for model.\n",
    "\n",
    "\n",
    "***Using different vectorizers is in progress.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4:  CNN with different vectorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model(hidden_layers=[SpatialDropout1D(0.5),\n",
    "                        Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'),\n",
    "                        BatchNormalization()],\n",
    "                      glove = False,\n",
    "                      embedding_matrix=None)\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/tokens/test_file.pickle\",\"rb\")\n",
    "test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data/tokens/train_file.pickle\",\"rb\")\n",
    "train = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train2=train.sample(n=80000, random_state=1)\n",
    "train2=train2.reset_index()\n",
    "test2=test.sample(n=20000, random_state=1)\n",
    "test2=test2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_split (train, test):\n",
    "    X_train=train.comment\n",
    "    X_test=test.comment\n",
    "    y_train=train.label\n",
    "    y_test=test.label\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split label and comment columns in the data\n",
    "X_train2, X_test2, y_train2, y_test2 = label_split(train2, test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "tfidfvec2 = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "tfidfvec3 = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))\n",
    "countvec = CountVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "countvec2 = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "countvec3 = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"cnn_tfidfvec\",\"cnn_tfidfvec2\", \"cnn_tfidfvec3\", \"cnn_countvec\", \"cnn_countvec2\", \"cnn_countvec3\"]\n",
    "vectorizers=[tfidfvec,tfidfvec2,tfidfvec3,countvec,countvec2,countvec3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def model_performance(vectorizer, train_data, test_data, y_test):\n",
    "accuracy_df = []\n",
    "for name, vectorizer in zip(names, vectorizers):\n",
    "\n",
    "    X_train2 = vectorizer.fit_transform(train2)\n",
    "    X_test2 = vectorizer.transform(test2)\n",
    "\n",
    "    print(\"Model Name :\", name)\n",
    "    print(\"======================\")\n",
    "\n",
    "    row=run_model(model, name, results, epochs)\n",
    "    results=save_results(results, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
