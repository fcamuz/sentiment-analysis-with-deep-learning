{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 3- Prediction\n",
    "\n",
    "This notebook consists the functions and code for scraping comments from Amazon website, pre-prosessing them and prediction with the model.  \n",
    "\n",
    "### CHRISP-DM phase\n",
    "\n",
    "Deployment phase for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 6.Deployment\n",
    "\n",
    "Generally this will mean deploying a code representation of the model into an operating system to score or categorize new unseen data as it arises and to create a mechanism for the use of that new information in the solution of the original business problem. Importantly, the code representation must also include all the data prep steps leading up to modeling so that the model will treat new raw data in the same manner as during model development.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Scraping Comments\n",
    "- 4.Pre-processing the New Data\n",
    "- 5.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model, Sequential, Input\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('cnn_2cnv.h5')\n",
    "\n",
    "model = joblib.load(\"models/joblib_RL_Model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n",
    "\n",
    "Open the saved tokenizer with pickle. This tokenizer was trained in the pre-prosessing notebook and saved with pickle. We need this for creating the pipe line for the new data we will scrape from Amazon website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews ():\n",
    "    \"\"\"This function scrapes customer reviews from Amazon web \n",
    "    site at a given product page URL. Uses input method to receive\n",
    "    the URL from the user\"\"\"\n",
    "    # For ignoring SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    url=input(\"Enter Amazon Product Url- \")\n",
    "    html_page = urllib.request.urlopen(url) #Make a get request to retrieve the page\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    reviews=[]\n",
    "    ratings=[]\n",
    "\n",
    "    review_row=soup.findAll('div', attrs={'data-hook': 'review'})\n",
    "    for row in review_row:\n",
    "        ratings.append(row.find('span',  attrs={'class':'a-icon-alt'}).text.strip()[0])\n",
    "        reviews.append(row.find('div', attrs={'data-hook': 'review-collapsed'}).text.strip())\n",
    "    \n",
    "    print('There are {} reviews in for this product on this page'.format(len(reviews)) )\n",
    "    #print(reviews)\n",
    "    \n",
    "    return reviews, ratings\n",
    "\n",
    "def punctuationRemover(p):\n",
    "    '''\n",
    "    Input: Takes a string. You may have to use str() to force it. \n",
    "    Removes all punctuation by checking every single character.\n",
    "    Output: Returns a string.\n",
    "    '''\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890''' \n",
    "    no_punctuations = ''\n",
    "\n",
    "    for words in p: # You may not have to loop this high\n",
    "        for char in words:\n",
    "            if char in punctuations:\n",
    "                no_punctuations = no_punctuations + ' '\n",
    "            if char not in punctuations:\n",
    "                no_punctuations = no_punctuations + char    \n",
    "    return(no_punctuations)\n",
    "\n",
    "\n",
    "def removeStopWords(str):\n",
    "    \"\"\"it takes a string and removes the stopwords from it. \n",
    "    Stopwords are available in the \"stop\" list\"\"\"\n",
    "    #select english stopwords\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    #add custom words\n",
    "    stop.update(('arnt','this','when','cant','these'))\n",
    "    #remove stop words\n",
    "    new_str = ' '.join([word.lower() for word in str if word.lower() not in stop]) \n",
    "    return new_str\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Comments \n",
    "\n",
    "In this part we will scrape the comments from the website for a product. We will also scrape the rating to evaluate the model's prediction fro unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list, ratings = scrape_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_list, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-process the New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopwords and punctuation from the comment by using the pre-processing functions. Also split in to words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "review_list=['This is for practicing your chords or fingering for a guitar, and does not make a pleasant sound if strummed. This is a neat tool to use on the go, if youre waiting in line at the DMV, passing time in a waiting room or something like that. Its great in the sense that you dont have to disturb other people around you while pracitcing.','Great product! Comfortable to use!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_punc=[punctuationRemover(p) for p in review_list]\n",
    "review_list_no_punc=[nltk.word_tokenize(words) for words in review_list_no_punc]\n",
    "review_list_no_stop=[removeStopWords(p) for p in review_list_no_punc]\n",
    "review_list_no_stop=[nltk.word_tokenize(words) for words in review_list_no_stop]\n",
    "review_list_lemm=[removeStopWords(p) for p in review_list_no_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=review_list_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vektor=tokenizer.texts_to_sequences(test)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict([test_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([test_vector]))\n",
    "print([round(prediction[0][0]) for prediction[0][0] in prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and predicting multiple customer reviews from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_comments ():\n",
    "    #scrape comments from the website\n",
    "    review_list, ratings = scrape_reviews() \n",
    "    #pre-process the data\n",
    "    review_list_no_punc=[punctuationRemover(p) for p in review_list]\n",
    "    review_list_no_punc=[nltk.word_tokenize(words) for words in review_list_no_punc]\n",
    "    review_list_no_stop=[removeStopWords(p) for p in review_list_no_punc]\n",
    "    review_list_no_stop=[nltk.word_tokenize(words) for words in review_list_no_stop]\n",
    "    review_list_lemm=[removeStopWords(p) for p in review_list_no_punc]\n",
    "    \n",
    "    test=review_list_lemm\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    prediction=model.predict([test_vector])\n",
    "\n",
    "    neg_com=[]\n",
    "    pos_com=[]\n",
    "    label=[int(round(prediction[0][0])) for prediction[0][0] in prediction]\n",
    "    labels=[]\n",
    "    for i in label:\n",
    "        if i==1:\n",
    "            labels.append('P')\n",
    "        else: \n",
    "            labels.append('N')\n",
    "        \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='N':\n",
    "            neg_com.append(review_list[j])\n",
    "            \n",
    "            \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='P':\n",
    "            pos_com.append(review_list[j])\n",
    "   \n",
    "    print (\"\")\n",
    "    print (\"Predictions\")\n",
    "    print(labels)\n",
    "    print (\"Actual Rates\")\n",
    "    print(ratings)\n",
    "    print (\"\")\n",
    "    \n",
    "    print(\"List of negative comments\")\n",
    "    print(\"============================\")\n",
    "    for i in neg_com:\n",
    "        print (i)\n",
    "        print (\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the tone of a single comment such as email or text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#review_list=['This is for practicing your chords or fingering for a guitar, and does not make a pleasant sound if strummed. This is a neat tool to use on the go, if youre waiting in line at the DMV, passing time in a waiting room or something like that. Its great in the sense that you dont have to disturb other people around you while pracitcing.','Great product! Comfortable to use!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_comment ():\n",
    "    review_list=[input(\"Enter Text - \")]\n",
    "    review_list_no_punc=[punctuationRemover(p) for p in review_list]\n",
    "    review_list_no_punc=[nltk.word_tokenize(words) for words in review_list_no_punc]\n",
    "    review_list_no_stop=[removeStopWords(p) for p in review_list_no_punc]\n",
    "    review_list_no_stop=[nltk.word_tokenize(words) for words in review_list_no_stop]\n",
    "    review_list_lemm=[lemmatize_verbs(p) for p in review_list_no_punc]\n",
    "    test=review_list_no_stop\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    #test_vector\n",
    "    prediction=model.predict([test_vector])\n",
    "    #print(model.predict([test_vector]))\n",
    "    #print([round(prediction[0][0]) for prediction[0][0] in prediction])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\" \")\n",
    "    \n",
    "    if model.predict([test_vector])<0.4:\n",
    "        print('This might be a negative comment!')\n",
    "    elif model.predict([test_vector])>0.6:\n",
    "        print('This might be a positive comment!')\n",
    "    else :\n",
    "        print('This might be a neutral comment!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_comment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Text - Well I accidentally broke it in a matter of minutes after getting it out of the box. So go easy when tightening the strings. It comes with extra strings to replace them but there are no instructions of how to replace them and for the life of me I can't figure it out.  ***Update!*** The company was great and sent me a new one free of charge. I went easy on the tightening this time and now I have a great way to practice my chords and condition my fingers. It's also just a nice way to keep your fingers and hands busy. I quit smoking a few months ago and this is nice productive use of my hands.\n",
      "------------------------------------------\n",
      " \n",
      "This might be a positive comment!\n"
     ]
    }
   ],
   "source": [
    "single_comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a function that scrapes comments from a product page on Amazon website.\n",
    "\n",
    "You can easily use this model for classification for any amazon product. \n",
    "All you need is;\n",
    "\n",
    "- Run the web-comments  function\n",
    "- It will ask you to input the amazon product page URL\n",
    "- Just copy and paste the url and press ENTER\n",
    "\n",
    "The function will give you how many comments are there in total. It also labels each one of them for you as P or N. \n",
    "And filters the Negative one so that you would see why your customer is complaining .\n",
    "\n",
    "To get valuable insight to revise your customer service you would read and  analyze  only 3 comments instead of 20.\n",
    "\n",
    "I provides huge time and money saving in the long run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "***Work in progress***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Web App for the model \n",
    "\n",
    "\n",
    "\n",
    "For customer use I would  Create a web app out of this prediction model.\n",
    "Web scraping can be adapted to any product website other than Amazon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Connecting the prediction to Anvil \n",
    "I would like to run this prediction as a web app. I used one interface to get input and give the results to the user. It is called Anvil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"O5X2QNJXLPWEQ2MQIAJTAQHP-AYZTXNLHKK3ZZOB2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def say_hello(name):\n",
    "  print(\"Hello from the uplink, %s!\" % name)\n",
    "\n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "@anvil.server.collable\n",
    "def sentiment(file):\n",
    "    with anvil.media.Tempfile(file) as filename:\n",
    "        text = url_box(filename)       \n",
    "    test_vektor=tokenizer.texts_to_sequences(text)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    score=model.predict(text_vector)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ipython nbconvert --to FORMAT notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem in the connection. It kept throwing errors on the Anvil platform. So moved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creatin app with SPYRE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyre import server\n",
    "app=server.App()\n",
    "app.Launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a version missmatch here I guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MLFlow for machine learnig life cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to redo this project in this a platform to see the difference and also for easy deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
