{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 3- Prediction\n",
    "\n",
    "This notebook consists the functions and code for scraping comments from Amazon website, pre-prosessing them and prediction with the model.  \n",
    "\n",
    "### CHRISP-DM phase\n",
    "\n",
    "Deployment phase for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 6.Deployment\n",
    "\n",
    "Generally this will mean deploying a code representation of the model into an operating system to score or categorize new unseen data as it arises and to create a mechanism for the use of that new information in the solution of the original business problem. Importantly, the code representation must also include all the data prep steps leading up to modeling so that the model will treat new raw data in the same manner as during model development.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Scraping Comments\n",
    "- 4.Pre-processing the New Data\n",
    "- 5.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model, Sequential, Input\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('cnn_2cnv.h5')\n",
    "\n",
    "model = joblib.load(\"models/joblib_RL_Model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n",
    "\n",
    "Open the saved tokenizer with pickle. This tokenizer was trained in the pre-prosessing notebook and saved with pickle. We need this for creating the pipe line for the new data we will scrape from Amazon website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_reviews ():\n",
    "    # For ignoring SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    url=input(\"Enter Amazon Product Url- \")\n",
    "    html_page = urllib.request.urlopen(url) #Make a get request to retrieve the page\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    reviews=[]\n",
    "    ratings=[]\n",
    "\n",
    "    review_row=soup.findAll('div', attrs={'data-hook': 'review'})\n",
    "    for row in review_row:\n",
    "        ratings.append(row.find('span',  attrs={'class':'a-icon-alt'}).text.strip()[0])\n",
    "        reviews.append(row.find('div', attrs={'data-hook': 'review-collapsed'}).text.strip())\n",
    "    \n",
    "    print('There are {} reviews in for this product on this page'.format(len(reviews)) )\n",
    "    #print(reviews)\n",
    "    \n",
    "    return reviews, ratings\n",
    "\n",
    "def punctuationRemover(p):\n",
    "    '''\n",
    "    Input: Takes a string. You may have to use str() to force it. \n",
    "    Removes all punctuation by checking every single character.\n",
    "    Output: Returns a string.\n",
    "    '''\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890''' \n",
    "    no_punctuations = ''\n",
    "\n",
    "    for words in p: # You may not have to loop this high\n",
    "        for char in words:\n",
    "            if char in punctuations:\n",
    "                no_punctuations = no_punctuations + ' '\n",
    "            if char not in punctuations:\n",
    "                no_punctuations = no_punctuations + char    \n",
    "    return(no_punctuations)\n",
    "\n",
    "def no_stopword (p):\n",
    "    token= ' '.join([word.lower() for word in p.split() if word.lower not in (stop)])\n",
    "    return token\n",
    "\n",
    "def removeStopWords(str):\n",
    "    #select english stopwords\n",
    "    cachedStopWords = set(stopwords.words(\"english\"))\n",
    "    #add custom words\n",
    "    cachedStopWords.update(('arnt','this','when','cant','these'))\n",
    "    #remove stop words\n",
    "    new_str = ' '.join([word.lower() for word in str if word.lower() not in cachedStopWords]) \n",
    "    return new_str\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Comments \n",
    "\n",
    "In this part we will scrape the comments from the website for a product. We will also scrape the rating to evaluate the model's prediction fro unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list, ratings = scrape_reviews()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_list, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-process the New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopwords and punctuation from the comment by using the pre-processing functions. Also split in to words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_punc=[punctuationRemover(p) for p in review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_stop=[no_stopword(p) for p in review_list_no_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=review_list_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vektor=tokenizer.texts_to_sequences(test)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict([test_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([test_vector]))\n",
    "print([round(prediction[0][0]) for prediction[0][0] in prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and predicting multiple customer reviews from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_comments ():\n",
    "    review_list, ratings = scrape_reviews() \n",
    "    review_list_no_punc=[punctuationRemover(p) for p in review_list]\n",
    "    review_list_no_stop=[no_stopword(p) for p in review_list_no_punc]\n",
    "    test=review_list_no_stop\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    prediction=model.predict([test_vector])\n",
    "    \n",
    "    neg_com=[]\n",
    "    pos_com=[]\n",
    "    label=[int(round(prediction[0][0])) for prediction[0][0] in prediction]\n",
    "    labels=[]\n",
    "    for i in label:\n",
    "        if i==1:\n",
    "            labels.append('P')\n",
    "        else: \n",
    "            labels.append('N')\n",
    "        \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='N':\n",
    "            neg_com.append(review_list[j])\n",
    "            \n",
    "            \n",
    "    for j,i in enumerate(labels):\n",
    "        if i=='P':\n",
    "            pos_com.append(review_list[j])\n",
    "   \n",
    "    print (\"\")\n",
    "    print (\"Predictions\")\n",
    "    print(labels)\n",
    "    print (\"Actual Rates\")\n",
    "    print(ratings)\n",
    "    print (\"\")\n",
    "    \n",
    "    print(\"List of negative comments\")\n",
    "    print(\"============================\")\n",
    "    for i in neg_com:\n",
    "        print (i)\n",
    "        print (\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Amazon Product Url- https://www.amazon.com/Dexas-Popware-Collapsible-KlipScoop-Capacity/dp/B00N46S0IQ/ref=sr_1_2?keywords=cups&qid=1578342348&s=pet-supplies&sr=1-2&th=1\n",
      "There are 8 reviews in for this product on this page\n",
      "\n",
      "Predictions\n",
      "['P', 'P', 'P', 'N', 'P', 'P', 'P', 'P']\n",
      "Actual Rates\n",
      "['5', '4', '5', '1', '5', '4', '5', '5']\n",
      "\n",
      "List of negative comments\n",
      "============================\n",
      "I've had this for a few months and got it specifically for the clip so I could clip the cup to the bag instead of having it sitting in the bag with the food. in the first couple of weeks, the hinge pin slipped out of the hole. we fixed it, but this continues to happen almost weekly. it's really annoying.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "web_comments()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a function that scrapes comments from a product page on Amazon website.\n",
    "\n",
    "You can easily use this model for classification for any amazon product. \n",
    "All you need is;\n",
    "\n",
    "- Run the web-comments  function\n",
    "- It will ask you to input the amazon product page URL\n",
    "- Just copy and paste the url and press ENTER\n",
    "\n",
    "The function will give you how many comments are there in total. It also labels each one of them for you as P or N. \n",
    "And filters the Negative one so that you would see why your customer is complaining .\n",
    "\n",
    "To get valuable insight to revise your customer service you would read and  analyze  only 3 comments instead of 20.\n",
    "\n",
    "I provides huge time and money saving in the long run\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "***Work in progress***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the tone of a single comment such as email or text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_comment ():\n",
    "    comment=input(\"Enter comment here- \")\n",
    "    \n",
    "    comment_nopunc=[punctuationRemover(comment)]\n",
    "    comment_no_stop= \"\".join([word.lower() for word in comment_nopunc if  word.lower() not in (stop)])\n",
    "    print(comment_no_stop)\n",
    "\n",
    "    test=comment_no_stop\n",
    "    test_vektor=tokenizer.texts_to_sequences(test)\n",
    "    print(test_vektor)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    print(test_vector)\n",
    "    prediction=model.predict([test_vector])\n",
    "\n",
    "    print (int(round(prediction[0][0])))\n",
    "\n",
    "single_comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment=input(\"Enter comment here- \")\n",
    "\n",
    "comment=[\"I love this product!\"]\n",
    "    \n",
    "comment_nopunc=[punctuationRemover(comment)]\n",
    "comment_no_stop= \"\".join([word.lower() for word in comment_nopunc if  word.lower() not in (stop)])\n",
    "print(comment_no_stop)\n",
    "\n",
    "test=comment_no_stop\n",
    "test_vektor=tokenizer.texts_to_sequences (test)                             \n",
    "print(test_vektor)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "prediction=model.predict([test_vector])\n",
    "\n",
    "print (int(round(prediction[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Web App for the model \n",
    "\n",
    "***Work in progress***\n",
    "\n",
    "For customer use I would  Create a web app out of this prediction model.\n",
    "Web scraping can be adapted to any product website other than Amazon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the prediction to Anvil \n",
    "I would like to run this prediction as a web app. I used one interface to get input and give the results to the user. It is called Anvil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"O5X2QNJXLPWEQ2MQIAJTAQHP-AYZTXNLHKK3ZZOB2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def say_hello(name):\n",
    "  print(\"Hello from the uplink, %s!\" % name)\n",
    "\n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "@anvil.server.collable\n",
    "def sentiment(file):\n",
    "    with anvil.media.Tempfile(file) as filename:\n",
    "        text = url_box(filename)\n",
    "        \n",
    "        \n",
    "    test_vektor=tokenizer.texts_to_sequences(text)\n",
    "    test_vector=sequence.pad_sequences(test_vektor, maxlen=128)\n",
    "    \n",
    "    score=model.predict(text_vector)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ipython nbconvert --to FORMAT notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem in the connection. It kept throwing errors on the Anvil platform. So moved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatin app with SPYRE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyre import server\n",
    "\n",
    "app=server.App()\n",
    "\n",
    "app.Launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spyre\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement spyre (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for spyre\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a version missmatch here I guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLFlow for machine learnig life cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to redo this project in this a platform to see the difference and also for easy deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
