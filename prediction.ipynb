{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "# Phase 3- Prediction\n",
    "\n",
    "This notebook consists the functions and code for scraping comments from Amazon website, pre-prosessing them and prediction with the model.  \n",
    "\n",
    "### CHRISP-DM phase\n",
    "\n",
    "Deployment phase for CRISP-DM can be found in this noteboook.\n",
    "\n",
    "#### 6.Deployment\n",
    "\n",
    "Generally this will mean deploying a code representation of the model into an operating system to score or categorize new unseen data as it arises and to create a mechanism for the use of that new information in the solution of the original business problem. Importantly, the code representation must also include all the data prep steps leading up to modeling so that the model will treat new raw data in the same manner as during model development.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- 1.Import Libraries\n",
    "- 2.Define Functions \n",
    "- 3.Scraping Comments\n",
    "- 4.Pre-processing the New Data\n",
    "- 5.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model, Sequential, Input\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('cnn_2cnv.h5')\n",
    "\n",
    "model = joblib.load(\"models/joblib_RL_Model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n",
    "\n",
    "Open the saved tokenizer with pickle. This tokenizer was trained in the pre-prosessing notebook and saved with pickle. We need this for creating the pipe line for the new data we will scrape from Amazon website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_reviews ():\n",
    "    # For ignoring SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    url=input(\"Enter Amazon Product Url- \")\n",
    "    html_page = urllib.request.urlopen(url) #Make a get request to retrieve the page\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "    reviews=[]\n",
    "    ratings=[]\n",
    "\n",
    "    review_row=soup.findAll('div', attrs={'data-hook': 'review'})\n",
    "    for row in review_row:\n",
    "        ratings.append(row.find('span',  attrs={'class':'a-icon-alt'}).text.strip()[0])\n",
    "        reviews.append(row.find('div', attrs={'data-hook': 'review-collapsed'}).text.strip())\n",
    "    \n",
    "    print('There are {} reviews in for this product'.format(len(reviews)) )\n",
    "    print(reviews)\n",
    "    \n",
    "    return reviews, ratings\n",
    "\n",
    "def punctuationRemover(p):\n",
    "    '''\n",
    "    Input: Takes a string. You may have to use str() to force it. \n",
    "    Removes all punctuation by checking every single character.\n",
    "    Output: Returns a string.\n",
    "    '''\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890''' \n",
    "    no_punctuations = ''\n",
    "\n",
    "    for words in p: # You may not have to loop this high\n",
    "        for char in words:\n",
    "            if char in punctuations:\n",
    "                no_punctuations = no_punctuations + ' '\n",
    "            if char not in punctuations:\n",
    "                no_punctuations = no_punctuations + char    \n",
    "    return(no_punctuations)\n",
    "\n",
    "def no_stopword (p):\n",
    "    token= ' '.join([word.lower() for word in p.split() if word.lower not in (stop)])\n",
    "    return token\n",
    "\n",
    "def removeStopWords(str):\n",
    "    #select english stopwords\n",
    "    cachedStopWords = set(stopwords.words(\"english\"))\n",
    "    #add custom words\n",
    "    cachedStopWords.update(('arnt','this','when','cant','these'))\n",
    "    #remove stop words\n",
    "    new_str = ' '.join([word.lower() for word in str if word.lower() not in cachedStopWords]) \n",
    "    return new_str\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Comments \n",
    "\n",
    "In this part we will scrape the comments from the website for a product. We will also scrape the rating to evaluate the model's prediction fro unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list, ratings = scrape_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_list, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-process the New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopwords and punctuation from the comment by using the pre-processing functions. Also split in to words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_punc=[punctuationRemover(p) for p in review_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_no_stop=[no_stopword(p) for p in review_list_no_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=review_list_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vektor=tokenizer.texts_to_sequences(test)\n",
    "test_vector=sequence.pad_sequences(test_vektor, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict([test_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([test_vector]))\n",
    "print([round(prediction[0][0]) for prediction[0][0] in prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
